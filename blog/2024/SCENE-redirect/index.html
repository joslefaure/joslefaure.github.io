<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h3 id="paper--code"> <a href="https://github.com/joslefaure/SCENE" rel="external nofollow noopener" target="_blank">Paper</a> / <a href="https://github.com/joslefaure/SCENE" rel="external nofollow noopener" target="_blank">Code</a> </h3> <h2 id="abstract">Abstract</h2> <p>While existing research often treats long-form videos as extended short videos, we propose a novel approach that more accurately reflects human cognition. This paper introduces <strong>SCENE</strong>: <strong>S</strong>emanti<strong>C</strong> and <strong>E</strong>pisodic <strong>NE</strong>twork for Long-Form Video Understanding, a model that simulates episodic memory accumulation to capture action sequences and reinforces them with semantic knowledge dispersed throughout the video. Our work makes two key contributions: First, we develop an Episodic Memory ComprEssor (EMCEE) module that efficiently aggregates crucial representations from micro to semi-macro levels. Second, we propose a Semantic Knowledge ExtrAcTor (SKEAT) that enhances these aggregated representations with semantic information by focusing on the broader context, dramatically reducing feature dimensionality while preserving relevant macro-level information. Extensive experiments demonstrate that <strong>SCENE</strong> achieves state-of-the-art performance across multiple long-video understanding benchmarks in both zero-shot and fully-supervised settings. Our code will be made public at <a href="https://github.com/joslefaure/SCENE" rel="external nofollow noopener" target="_blank">SCENE</a>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/intuition-480.webp 480w,/assets/img/intuition-800.webp 800w,/assets/img/intuition-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/intuition.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="main-contributions">Main Contributions</h2> <p>Our key contributions are as follows:</p> <ul> <li>We propose an Episodic Memory ComprEssor (EMCEE) to stream through the video and keep important episodes by aggregating similar scenes. EMCEE enables efficient processing of extended video sequences while preserving temporal coherence and narrative structure.</li> <li>We develop a Semantic Knowledge Extractor (SKEAT) that enhances the modelâ€™s understanding of long videos by distilling high-level semantic cues, providing a cohesive framework for understanding the context and themes within long-form videos.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/method-480.webp 480w,/assets/img/method-800.webp 800w,/assets/img/method-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/method.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="results-and-ablations">Results and Ablations</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/results-480.webp 480w,/assets/img/results-800.webp 800w,/assets/img/results-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/results.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="qualitative-results">Qualitative Results</h2> <h2 id="citation">Citation</h2> </body></html>