<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://joslefaure.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://joslefaure.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-04T03:46:38+00:00</updated><id>https://joslefaure.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Longing</title><link href="https://joslefaure.github.io/blog/2025/longing/" rel="alternate" type="text/html" title="Longing"/><published>2025-02-03T09:24:00+00:00</published><updated>2025-02-03T09:24:00+00:00</updated><id>https://joslefaure.github.io/blog/2025/longing</id><content type="html" xml:base="https://joslefaure.github.io/blog/2025/longing/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Time’s Ruthless Dance</title><link href="https://joslefaure.github.io/blog/2025/times-ruthless-dance/" rel="alternate" type="text/html" title="Time’s Ruthless Dance"/><published>2025-01-19T09:50:04+00:00</published><updated>2025-01-19T09:50:04+00:00</updated><id>https://joslefaure.github.io/blog/2025/times-ruthless-dance</id><content type="html" xml:base="https://joslefaure.github.io/blog/2025/times-ruthless-dance/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">A Gentle Introduction to Diffusion Models</title><link href="https://joslefaure.github.io/blog/2025/a-gentle-introduction-to-diffusion-models/" rel="alternate" type="text/html" title="A Gentle Introduction to Diffusion Models"/><published>2025-01-13T10:07:42+00:00</published><updated>2025-01-13T10:07:42+00:00</updated><id>https://joslefaure.github.io/blog/2025/a-gentle-introduction-to-diffusion-models</id><content type="html" xml:base="https://joslefaure.github.io/blog/2025/a-gentle-introduction-to-diffusion-models/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">My Double Life as a Full-Time Employee and PhD Student</title><link href="https://joslefaure.github.io/blog/2024/my-double-life-as-a-full-time-employee-and-phd-student/" rel="alternate" type="text/html" title="My Double Life as a Full-Time Employee and PhD Student"/><published>2024-12-31T03:46:00+00:00</published><updated>2024-12-31T03:46:00+00:00</updated><id>https://joslefaure.github.io/blog/2024/my-double-life-as-a-full-time-employee-and-phd-student</id><content type="html" xml:base="https://joslefaure.github.io/blog/2024/my-double-life-as-a-full-time-employee-and-phd-student/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">MovieCORE</title><link href="https://joslefaure.github.io/blog/2024/MOVIECORE/" rel="alternate" type="text/html" title="MovieCORE"/><published>2024-12-09T11:06:00+00:00</published><updated>2024-12-09T11:06:00+00:00</updated><id>https://joslefaure.github.io/blog/2024/MOVIECORE</id><content type="html" xml:base="https://joslefaure.github.io/blog/2024/MOVIECORE/"><![CDATA[]]></content><author><name></name></author><category term="paper"/><category term="video-understanding"/><category term="llm"/><category term="paper"/><category term="llm-agents"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Trying to find God</title><link href="https://joslefaure.github.io/blog/2024/trying-to-find-god/" rel="alternate" type="text/html" title="Trying to find God"/><published>2024-12-06T02:32:03+00:00</published><updated>2024-12-06T02:32:03+00:00</updated><id>https://joslefaure.github.io/blog/2024/trying-to-find-god</id><content type="html" xml:base="https://joslefaure.github.io/blog/2024/trying-to-find-god/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Turning 29</title><link href="https://joslefaure.github.io/blog/2024/turning-29/" rel="alternate" type="text/html" title="Turning 29"/><published>2024-08-02T09:34:35+00:00</published><updated>2024-08-02T09:34:35+00:00</updated><id>https://joslefaure.github.io/blog/2024/turning-29</id><content type="html" xml:base="https://joslefaure.github.io/blog/2024/turning-29/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Let’s demystify overfitting</title><link href="https://joslefaure.github.io/blog/2024/lets-demystify-overfitting/" rel="alternate" type="text/html" title="Let’s demystify overfitting"/><published>2024-07-19T07:41:02+00:00</published><updated>2024-07-19T07:41:02+00:00</updated><id>https://joslefaure.github.io/blog/2024/lets-demystify-overfitting</id><content type="html" xml:base="https://joslefaure.github.io/blog/2024/lets-demystify-overfitting/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">HERMES</title><link href="https://joslefaure.github.io/blog/2024/SCENE-redirect/" rel="alternate" type="text/html" title="HERMES"/><published>2024-07-15T11:06:00+00:00</published><updated>2024-07-15T11:06:00+00:00</updated><id>https://joslefaure.github.io/blog/2024/SCENE-redirect</id><content type="html" xml:base="https://joslefaure.github.io/blog/2024/SCENE-redirect/"><![CDATA[<h3 id="paper--code"><a href="https://github.com/joslefaure/SCENE">Paper</a> / <a href="https://github.com/joslefaure/SCENE">Code</a></h3> <h2 id="abstract">Abstract</h2> <p>While existing research often treats long-form videos as extended short videos, we propose a novel approach that more accurately reflects human cognition. This paper introduces <strong>SCENE</strong>: <strong>S</strong>emanti<strong>C</strong> and <strong>E</strong>pisodic <strong>NE</strong>twork for Long-Form Video Understanding, a model that simulates episodic memory accumulation to capture action sequences and reinforces them with semantic knowledge dispersed throughout the video. Our work makes two key contributions: First, we develop an Episodic Memory ComprEssor (EMCEE) module that efficiently aggregates crucial representations from micro to semi-macro levels. Second, we propose a Semantic Knowledge ExtrAcTor (SKEAT) that enhances these aggregated representations with semantic information by focusing on the broader context, dramatically reducing feature dimensionality while preserving relevant macro-level information. Extensive experiments demonstrate that <strong>SCENE</strong> achieves state-of-the-art performance across multiple long-video understanding benchmarks in both zero-shot and fully-supervised settings. Our code will be made public at <a href="https://github.com/joslefaure/SCENE">SCENE</a>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/intuition-480.webp 480w,/assets/img/intuition-800.webp 800w,/assets/img/intuition-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/intuition.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="main-contributions">Main Contributions</h2> <p>Our key contributions are as follows:</p> <ul> <li>We propose an Episodic Memory ComprEssor (EMCEE) to stream through the video and keep important episodes by aggregating similar scenes. EMCEE enables efficient processing of extended video sequences while preserving temporal coherence and narrative structure.</li> <li>We develop a Semantic Knowledge Extractor (SKEAT) that enhances the model’s understanding of long videos by distilling high-level semantic cues, providing a cohesive framework for understanding the context and themes within long-form videos.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/method-480.webp 480w,/assets/img/method-800.webp 800w,/assets/img/method-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/method.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="results-and-ablations">Results and Ablations</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/results-480.webp 480w,/assets/img/results-800.webp 800w,/assets/img/results-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/results.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="qualitative-results">Qualitative Results</h2> <h2 id="citation">Citation</h2>]]></content><author><name>First Author Name</name></author><category term="paper"/><category term="video-understanding"/><category term="llm"/><category term="paper"/><summary type="html"><![CDATA[A novel method that simulates episodic memory accumulation to capture action sequences and reinforces them with semantic knowledge dispersed throughout the video.]]></summary></entry><entry><title type="html">A gentle introduction to the Kalman Filter</title><link href="https://joslefaure.github.io/blog/2023/a-gentle-introduction-to-the-kalman-filter/" rel="alternate" type="text/html" title="A gentle introduction to the Kalman Filter"/><published>2023-02-01T10:21:16+00:00</published><updated>2023-02-01T10:21:16+00:00</updated><id>https://joslefaure.github.io/blog/2023/a-gentle-introduction-to-the-kalman-filter</id><content type="html" xml:base="https://joslefaure.github.io/blog/2023/a-gentle-introduction-to-the-kalman-filter/"><![CDATA[]]></content><author><name></name></author></entry></feed>