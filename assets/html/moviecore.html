<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="description" content="Cognitive Reasoning in Movies" />
    <meta name="keywords" content="LLM-agent, Video-Understanding, Reasoning" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>MovieCORE: COgnitive REasoning in Movies</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-RM27MDSL17"></script>
    <script>
      window.dataLayer = window.dataLayer || [];

      function gtag() {
        dataLayer.push(arguments);
      }

      gtag("js", new Date());

      gtag("config", "G-PYVRSFMDRL");
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />

    <link rel="stylesheet" href="../static/css/bulma.min.css" />
    <link rel="stylesheet" href="../static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="../static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="../static/css/fontawesome.all.min.css" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
    <link rel="stylesheet" href="../static/css/index.css" />
    <link rel="icon" href="../img/favicon_ai.jpeg" />

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="../static/js/fontawesome.all.min.js"></script>
    <script src="../static/js/bulma-carousel.min.js"></script>
    <script src="../static/js/bulma-slider.min.js"></script>
    <script src="../static/js/index.js"></script>
  </head>
  <body>
    <nav class="navbar" role="navigation" aria-label="main navigation">
      <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
        </a>
      </div>
      <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center">
          <a class="navbar-item" href="https://keunhong.com">
            <span class="icon">
              <i class="fas fa-home"></i>
            </span>
          </a>

          <div class="navbar-item has-dropdown is-hoverable">
            <a class="navbar-link"> More Research </a>
            <div class="navbar-dropdown">
              <a
                class="navbar-item"
                href="https://openaccess.thecvf.com/content/WACV2023/html/Faure_Holistic_Interaction_Transformer_Network_for_Action_Detection_WACV_2023_paper.html"
              >
                HIT
              </a>
              <a class="navbar-item" href="https://arxiv.org/abs/2304.04688"> iCLIP </a>
              <a class="navbar-item" href="https://joslefaure.github.io/assets/html/hermes.html"> HERMES </a>
            </div>
          </div>
        </div>
      </div>
    </nav>
    <!-- Message -->
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">MovieCORE: COgnitive REasoning in Movies</h1>
              <h3 class="title is-4">EMNLP 2025 - Main Conference</h3>
              <div class="is-size-5 publication-authors">
                <span class="author-block"> <a href="https://joslefaure.github.io/">Gueter Josmy Faure</a><sup>1</sup>, </span>
                <span class="author-block"> <a href="">Ying Cheng</a><sup>4</sup>, </span>
                <span class="author-block"> <a href="https://minhungchen.netlify.app/">Min-Hung Chen</a><sup>2</sup>, </span>
                <span class="author-block"> <a href="https://www.cmlab.csie.ntu.edu.tw/~jiafongyeh/">Jia-Fong Yeh</a><sup>1</sup>, </span>
                <span class="author-block"> <a href="">Hung-Ting Su</a><sup>1</sup>, </span>
                <span class="author-block"> <a href="">Yung-Hao Tang</a><sup>5</sup>, </span>
                <span class="author-block"> <a href="https://winstonhsu.info/">Winston H. Hsu</a><sup>1,3</sup>, </span>
                <span class="author-block"> <a href="">Shang-Hong Lai</a><sup>4</sup></span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block"><sup>1</sup>National Taiwan University, </span>
                <span class="author-block"><sup>2</sup>NVIDIA, </span>
                <span class="author-block"><sup>3</sup>Mobile Drive Technology, </span>
                <span class="author-block"><sup>4</sup>National Tsing Hua University</span>
                <span class="author-block"><sup>5</sup>National Chengchi University</span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- PDF Link. -->
                  <span class="link-block">
                    <a href="https://openreview.net/pdf?id=pDsr2Dh0JG" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>
                  <!-- Video Link. -->
                  <span class="link-block">
                    <a href="" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-youtube"></i>
                      </span>
                      <span>Video</span>
                    </a>
                  </span>
                  <!-- Code Link. -->
                  <span class="link-block">
                    <a href="https://github.com/joslefaure/moviecore" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                  <!-- Dataset Link. -->
                  <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span> -->
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img src="../static/images/moviecore/poster_teaser.png" alt="Teaser image." />
          <h2 class="subtitle has-text-centered">
            The <span class="dnerf">MovieCORE</span> dataset is specifically created to challenge VLMs to deeply understand movies.
            <!-- "The MovieCORE dataset leverages an innovative 'agentic annotation' pipeline to automate the creation of thought-provoking video questions that challenge AI's deeper cognitive understanding." -->
          </h2>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                This paper introduces MovieCORE, a novel video question answering (VQA) dataset designed to probe deeper cognitive understanding of
                movie content.
              </p>
              <p>
                Unlike existing datasets that focus on surface-level comprehension, MovieCORE emphasizes thought-provoking questions that engage
                System-2 thinking while remaining specific to the video material. We propose an innovative agentic brainstorming approach, utilizing
                multiple large language models (LLMs) as thought agents to generate and refine highquality question-answer pairs. To evaluate dataset
                quality, we develop a set of cognitive tests assessing depth, thoughtprovocation potential, and syntactic complexity. We also propose
                a comprehensive evaluation scheme for assessing VQA model performance on deeper cognitive tasks. Our work contributes to advancing
                movie understanding in AI systems and provides valuable insights into the capabilities and limitations of current VQA models when
                faced with more challenging, nuanced questions about cinematic content. We will make our agentic annotation system, the dataset and
                its metadata publicly available.
              </p>
            </div>
          </div>
        </div>
        <!--/ Abstract. -->
      </div>
    </section>

    <section class="section">
      <div class="container is-max-desktop">
        <!-- Animation. -->
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">Agentic Annotation Workflow</h2>

            <div class="content has-text-justified">
              <p>
                The Critic Agent, acting as the master of ceremonies (MC), orchestrates interactions among specialized agents using video context and
                task instructions. It sequentially engages the System II VQA Expert, Skeptical Researcher, Detective, and Meta Reviewer, accumulating
                insights at each stage
              </p>
              <p>
                Upon receiving final recommendations from the Meta Reviewer, the MC relays them to the System II VQA Expert for VQA refinement.
                Subsequently, a subset of these refined VQAs undergoes evaluation by human experts for final validation.
              </p>
            </div>
            <div class="content has-text-centered">
              <img src="../static/images/moviecore/agenticWorkflow_acm_v3.png" alt="qualitative 2" />
            </div>

            <h2 class="title is-3">Comparison with other datasets in terms of semantic richness and depth</h2>

            <!-- Interpolating. -->
            <div class="content has-text-justified">
              <p><i>MovieCORE</i>, unlike other datasets priotizes system-2 thinking, leading to more depth in the QAs.</p>
            </div>
            <div class="publication-video">
              <img src="../static/images/moviecore/poster_dataset_comparison.png" alt="dataset Comparison." />
            </div>
            <!--/ Interpolating. -->

            <!-- Re-rendering. -->
            <h2 class="title is-3">Qualitative Comparison between Single-Pass and Agentic Annotations</h2>
            <div class="content has-text-centered">
              <img src="../static/images/moviecore/poster_agentic_vs_sp.png" alt="qualitative 2" />
            </div>
            <!--/ Re-rendering. -->

            <!-- Re-rendering. -->
            <div class="content has-text-centered">
              <img src="../static/images/moviecore/agentic_comparison_v2.png" alt="qualitative 2" />
            </div>
            <!--/ Re-rendering. -->

            <!-- Re-rendering. -->
            <div class="content has-text-centered">
              <img src="../static/images/moviecore/agentic_comparison_v3.png" alt="qualitative 3" />
            </div>
            <div class="content has-text-justified">
              <p>
                <span class="dnerf">Single-Pass versus Agentic Annotation.</span> The agentic method (bottom) elicits specific scene details, concrete
                examples, and detailed story elements, demonstrating the enhanced granularity achieved through multi-agent refinement.
              </p>
            </div>
            <!--/ Re-rendering. -->

            <!-- Re-rendering. -->
            <div class="content has-text-centered">
              <img src="../static/images/moviecore/vlm_results.png" alt="quantitative 1" />
            </div>
            <div class="content has-text-justified">
              <p>
                <span class="dnerf">Performance Comparison of Video Question-Answering Models on MovieCORE.</span> We evaluate various open-source and proprietary Vision-Language Models (VLMs) on five criteria: Accuracy, Comprehensiveness, Depth, Evidence, and Coherence. We use the 7B version of the open-source VLMs (8B for InternVL2.5).
              </p>
            </div>
            <!--/ Re-rendering. -->
          </div>
        </div>
      </div>
    </section>

    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@inproceedings{faure2moviecore,
          title={MovieCORE: COgnitive REasoning in Movies},
          author={Faure, Gueter Josmy and Chen, Min-Hung and Yeh, Jia-Fong and Cheng, Ying and Su, Hung-Ting and Lai, Shang-Hong and Hsu, Winston H},
          booktitle={The First Workshop on System-2 Reasoning at Scale, NeurIPS'24}
        }</code></pre>
      </div>
    </section>

    <footer class="footer">
      <div class="container">
        <div class="content has-text-centered">
          <a class="icon-link" href="https://arxiv.org/abs/2408.17443">
            <i class="fas fa-file-pdf"></i>
          </a>
          <a class="icon-link" href="https://github.com/joslefaure/HERMES" class="external-link" disabled>
            <i class="fab fa-github"></i>
          </a>
        </div>
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                This template is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, and licenced under a
                <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"
                  >Creative Commons Attribution-ShareAlike 4.0 International License</a
                >.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>
  </body>
</html>
